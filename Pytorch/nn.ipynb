{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.LayerNorm` 是 PyTorch 中用于应用层归一化的类。层归一化（Layer Normalization）是一种在深度神经网络中常用的技术，主要用于归一化输入的特征。下面是 `torch.nn.LayerNorm` 的详细参数解释：\n",
    "\n",
    "### 参数\n",
    "\n",
    "- **normalized_shape** (`int` 或 `tuple`): 输入张量的形状，从最后一个维度开始，应用归一化。例如，如果输入的特征张量形状为 `[batch_size, num_features, height, width]`，则 `normalized_shape` 可以是 `[num_features, height, width]` 或 `num_features`。\n",
    "- **eps** (`float`, 可选): 用于数值稳定性的一个小常数，防止除以零。默认值是 `1e-05`。\n",
    "- **elementwise_affine** (`bool`, 可选): 是否为每个元素添加可学习的缩放参数和偏移参数。如果设置为 `True`，则每个特征将有对应的缩放和偏移参数。默认为 `True`。\n",
    "- **bias** (`bool`, 可选): 是否添加偏置项（该参数已弃用，推荐使用 `elementwise_affine` 代替）。\n",
    "- **device** (`torch.device`, 可选): 选择运行LayerNorm的设备（例如：`'cuda:0'` 或 `'cpu'`）。如果未指定，则使用当前默认设备。\n",
    "- **dtype** (`torch.dtype`, 可选): 设置参数的数据类型。如果未指定，则使用默认的数据类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "# 输入张量的形状是 [batch_size, num_features, height, width]\n",
    "input = torch.randn(10, 20, 50, 50)\n",
    "\n",
    "# 创建 LayerNorm 对象\n",
    "layer_norm = LayerNorm([20, 50, 50])\n",
    "\n",
    "# 应用层归一化\n",
    "output = layer_norm(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.triu 函数的基本用法\n",
    "参数:\n",
    "input (Tensor)：输入的二维张量（矩阵）。\n",
    "diagonal (int, 可选)：控制哪一条对角线以上的元素应该被保留。\n",
    "diagonal=0：保留主对角线及其以上的元素。\n",
    "diagonal=1：保留主对角线上方的元素。\n",
    "diagonal=-1：保留主对角线及其下方一条对角线的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      " tensor([[0.9222, 0.7130, 0.2831, 0.4549],\n",
      "        [0.2785, 0.5111, 0.5648, 0.7722],\n",
      "        [0.5796, 0.2247, 0.2990, 0.4520],\n",
      "        [0.7229, 0.3689, 0.9810, 0.5729]])\n",
      "Upper Triangular Matrix:\n",
      " tensor([[0.9222, 0.7130, 0.2831, 0.4549],\n",
      "        [0.0000, 0.5111, 0.5648, 0.7722],\n",
      "        [0.0000, 0.0000, 0.2990, 0.4520],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5729]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(4, 4)  # 创建一个4x4的随机矩阵\n",
    "print(\"Original Matrix:\\n\", a)\n",
    "upper_tri = torch.triu(a)\n",
    "print(\"Upper Triangular Matrix:\\n\", upper_tri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，transpose 方法用于交换张量的两个维度。当你看到如 key.transpose(-2, -1) 的调用时，这里的 -2 和 -1 是指张量的维度索引，其中负数索引表示从后向前数。\n",
    "\n",
    "解释维度索引\n",
    "-1 代表最后一个维度。\n",
    "-2 代表倒数第二个维度。\n",
    "示例与应用\n",
    "假设我们有一个三维张量，形状为 (10, 20, 30)，表示10个20x30的矩阵。调用 transpose(-2, -1) 会交换最后两个维度，结果张量的形状将变为 (10, 30, 20)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor shape: torch.Size([3, 2, 3])\n",
      "Transposed tensor shape: torch.Size([3, 3, 2])\n",
      "\n",
      "Original tensor:\n",
      "tensor([[[ 0.4072,  0.9948, -0.0123],\n",
      "         [ 0.2405,  0.0720, -1.5082]],\n",
      "\n",
      "        [[-1.1210,  1.4243,  0.2197],\n",
      "         [-1.0906, -1.5585, -0.4028]],\n",
      "\n",
      "        [[ 0.9433, -0.0157,  0.1985],\n",
      "         [-1.2856, -0.6498,  0.1067]]])\n",
      "\n",
      "Transposed tensor:\n",
      "tensor([[[ 0.4072,  0.2405],\n",
      "         [ 0.9948,  0.0720],\n",
      "         [-0.0123, -1.5082]],\n",
      "\n",
      "        [[-1.1210, -1.0906],\n",
      "         [ 1.4243, -1.5585],\n",
      "         [ 0.2197, -0.4028]],\n",
      "\n",
      "        [[ 0.9433, -1.2856],\n",
      "         [-0.0157, -0.6498],\n",
      "         [ 0.1985,  0.1067]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a randomly initialized tensor with shape (3, 2, 1)\n",
    "tensor = torch.randn(3, 2, 3)\n",
    "\n",
    "# Transpose the last two dimensions\n",
    "transposed_tensor = tensor.transpose(-2, -1)\n",
    "\n",
    "# Print the shapes of the original and transposed tensors\n",
    "print(\"Original tensor shape:\", tensor.shape)  # Expected output: (3, 2, 1)\n",
    "print(\"Transposed tensor shape:\", transposed_tensor.shape)  # Expected output: (3, 1, 2)\n",
    "\n",
    "# Display the original and transposed tensors\n",
    "print(\"\\nOriginal tensor:\")\n",
    "print(tensor)\n",
    "print(\"\\nTransposed tensor:\")\n",
    "print(transposed_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "在深度学习模型中，我们常常会有一些离散的输入数据（如单词、ID 等），这些数据通常是整数表示的。`nn.Embedding` 模块将这些整数 ID 映射到连续的向量空间，以便这些离散的输入能够用于模型计算。\n",
    "\n",
    "### nn.Embedding 的基本定义\n",
    "\n",
    "```python\n",
    "nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)\n",
    "```\n",
    "参数详解\n",
    "num_embeddings：嵌入字典的大小。比如，对于一个词汇表，有 10,000 个不同的单词，那么 num_embeddings 就是 10,000。\n",
    "\n",
    "embedding_dim：每个嵌入向量的维度。例如，embedding_dim=100 表示每个单词或 ID 将被映射到一个 100 维的向量。\n",
    "\n",
    "padding_idx（可选）：指定一个索引，当输入为该索引时，嵌入的输出将为全零。通常用于处理变长序列中的填充（padding）情况，避免填充对模型训练产生影响。\n",
    "\n",
    "max_norm（可选）：若指定此参数，嵌入向量会在更新时被约束在一个最大范数的范围内，避免向量过大。\n",
    "\n",
    "norm_type（可选）：用于 max_norm 的范数类型，默认为 2（即 L2 范数）。\n",
    "\n",
    "scale_grad_by_freq（可选）：若为 True，则根据输入的频率对梯度进行缩放。输入频率越高，相应梯度越小，通常用于频率不均衡的数据。\n",
    "\n",
    "sparse（可选）：若为 True，则使用稀疏梯度，以节省内存。这对于大型嵌入字典非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "n, d, m = 3, 5, 7\n",
    "embedding = nn.Embedding(n, d, max_norm=1.0)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "loss = out.sigmoid().prod()\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aritfy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
