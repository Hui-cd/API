{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.GELU(approximate=\"tanh\") 激活函数主要有以下几个好处：\n",
    "\n",
    "平滑性与连续性：GELU（Gaussian Error Linear Unit）是一种平滑的激活函数，相比于ReLU，它更连续且没有突然的斜率变化。这有助于梯度流动，使模型更稳定，特别是在深层网络中避免梯度消失和爆炸问题。\n",
    "\n",
    "高斯概率解释：GELU 使用概率解释来保留输入值的一部分。与ReLU截断负值不同，GELU根据输入值大小按比例缩放和保留，接近正态分布。这使得在对输入有较强依赖的任务上（如自然语言处理中的BERT模型）效果更好。\n",
    "\n",
    "approximate=\"tanh\" 提升计算效率：GELU 的完整公式中包含复杂的计算（如误差函数），设置 approximate=\"tanh\" 使用了近似计算，采用 tanh 来近似，减少了计算量，从而加快了模型训练速度。尽管是近似计算，但对精度影响很小，因此适合大规模训练任务。\n",
    "\n",
    "实践效果：GELU 已被证明在许多实际任务中优于 ReLU 或 Leaky ReLU，特别是在NLP和Transformer架构中。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
